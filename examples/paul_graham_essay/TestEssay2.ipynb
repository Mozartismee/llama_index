{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a9eb90-335c-4214-8bb6-fd1edbe3ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My OpenAI Key\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a712b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f7baa-1c0a-430b-981b-83ddca9e71f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using GPT Tree Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0881f151-279e-4910-95c7-f49d3d6a4c69",
   "metadata": {},
   "source": [
    "#### [Demo] Default leaf traversal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d0b2364-4806-4656-81e7-3f6e4b910b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTTreeIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c297fd3-3424-41d8-9d0d-25fe6310ab62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "370fd08f-56ff-4c24-b0c4-c93116a6d482",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.common.tree.base:> Building index from nodes: 27 chunks\n",
      "> Building index from nodes: 27 chunks\n",
      "INFO:llama_index.indices.common.tree.base:> Building index from nodes: 2 chunks\n",
      "> Building index from nodes: 2 chunks\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total LLM token usage: 104012 tokens\n",
      "> [build_index_from_documents] Total LLM token usage: 104012 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "index = GPTTreeIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b4fe9b6-5762-4e86-b51e-aac45d3ecdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.save_to_disk('index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eec265d-211b-4f26-b05b-5b4e7072bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try loading\n",
    "new_index = GPTTreeIndex.load_from_disk('index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd14686d-1c53-4637-9340-3745f2121ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.query.tree.leaf_query:> Starting query: who is heidegger ?\n",
      "> Starting query: who is heidegger ?\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 0] Selected node: [2]/[2]\n",
      ">[Level 0] Selected node: [2]/[2]\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 1] Selected node: [2]/[2]\n",
      ">[Level 1] Selected node: [2]/[2]\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 2] Selected node: [2]/[2]\n",
      ">[Level 2] Selected node: [2]/[2]\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 7280 tokens\n",
      "> [query] Total LLM token usage: 7280 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\n",
      "> [query] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "response = new_index.query(\"who is heidegger ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4c87d14-d2d8-4d80-89f6-1e5972973528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Heidegger is a German philosopher who is best known for his book Being and Time. He is known for his concept of \"Dasein,\" which is the experience of being in the world. He also developed a relational ontology, which is the idea that entities are intelligible only in relation to other entities and the contexts in which they exist. Heidegger's concept of \"worlds\" refers to large-scale holistic networks of interconnected relational significance.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68c9ebfe-b1b6-4f4e-9278-174346de8c90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.query.tree.leaf_query:> Starting query: how to explaine intentionality？\n",
      "> Starting query: how to explaine intentionality？\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 1] Selected node: [1]/[1]\n",
      ">[Level 1] Selected node: [1]/[1]\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 2] Selected node: [1]/[1]\n",
      ">[Level 2] Selected node: [1]/[1]\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 7211 tokens\n",
      "> [query] Total LLM token usage: 7211 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\n",
      "> [query] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "response = new_index.query(\"how to explaine intentionality？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5ab5943-7c84-4c2b-ac99-ec4b5fc67e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Intentionality can be explained as the power of minds and mental states to represent, or stand for, things, properties, and states of affairs. This means that mental states, such as thoughts, can represent or stand for something that is not present in the immediate environment. For example, a thought or utterance about dinosaurs can represent or stand for a state of affairs that is far removed in space and time from the thought or utterance itself. This power of representation is what is meant by intentionality.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c62ec3-c3cf-467e-ab0f-88ffb9f990be",
   "metadata": {},
   "source": [
    "#### [Demo] Leaf traversal with child_branch_factor=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46714db4-9592-4c55-9ca7-916758f2ce68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.query.tree.leaf_query:> Starting query: which important reference was introduced by philosophy of encyclopedia to explaine intentionality?\n",
      "> Starting query: which important reference was introduced by philosophy of encyclopedia to explaine intentionality?\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 0] Selected node: [1]/[1,2]\n",
      ">[Level 0] Selected node: [1]/[1,2]\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 1] Selected node: [1]/[1,8]\n",
      ">[Level 1] Selected node: [1]/[1,8]\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 2] Selected node: [1]/[1,2]\n",
      ">[Level 2] Selected node: [1]/[1,2]\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 2] Selected node: [2]/[1,2]\n",
      ">[Level 2] Selected node: [2]/[1,2]\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 1] Selected node: [8]/[1,8]\n",
      ">[Level 1] Selected node: [8]/[1,8]\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 2] Selected node: [1]/[1,3]\n",
      ">[Level 2] Selected node: [1]/[1,3]\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 2] Selected node: [3]/[1,3]\n",
      ">[Level 2] Selected node: [3]/[1,3]\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 0] Selected node: [2]/[1,2]\n",
      ">[Level 0] Selected node: [2]/[1,2]\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 1] Selected node: [2]/[2,5]\n",
      ">[Level 1] Selected node: [2]/[2,5]\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 2] Selected node: [2]/[2,3]\n",
      ">[Level 2] Selected node: [2]/[2,3]\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 2] Selected node: [3]/[2,3]\n",
      ">[Level 2] Selected node: [3]/[2,3]\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 1] Selected node: [5]/[2,5]\n",
      ">[Level 1] Selected node: [5]/[2,5]\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 2] Selected node: [8]/[8,10]\n",
      ">[Level 2] Selected node: [8]/[8,10]\n",
      "INFO:llama_index.indices.query.tree.leaf_query:>[Level 2] Selected node: [10]/[8,10]\n",
      ">[Level 2] Selected node: [10]/[8,10]\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 28549 tokens\n",
      "> [query] Total LLM token usage: 28549 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\n",
      "> [query] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "# try using branching factor 2\n",
    "response = new_index.query(\"which important reference was introduced by philosophy of encyclopedia to explaine intentionality?\", child_branch_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ea7f891-b7e1-497a-a965-14201b220404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The important reference introduced by the philosophy of encyclopedia to explain intentionality is Franz Brentano in the last quarter of the nineteenth century. Brentano argued that intentionality is the distinguishing feature of mental phenomena, and that all mental phenomena have intentionality. He proposed that intentionality is a relational property of mental states, and that it is the relationship between the mental state and its object that gives the mental state its content. He also argued that intentionality is not a criterion of mental states, but rather a feature of them. He further suggested that intentionality can be naturalized, and that all mental phenomena exhibit intentionality. This view has been further developed by authors such as Dreyfus, Dummett, Evans, Field, Fodor, Føllesdal, Frege, Grice, Guttenplan, Haugeland, Husserl, Jacob, Jeshion, Kaplan, Kim, Kingsbury, Ryder, and Williford, Mally, McDowell, McGinn, Meinong, Mill, Millikan, Naccache, Dehaene, Neander, Parsons, Peacocke, Perry, Premack, Woodruff, Putnam, Quine, Recanati, Richard, Rorty, Rosenthal</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c572726-bb95-49c3-a762-d966de59ee5f",
   "metadata": {},
   "source": [
    "#### [Demo] Build Tree Index during Query-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255fb052-1ff6-4f27-881f-28d4790e9520",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85371256-292c-473e-9485-7de5c1997a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_light = GPTTreeIndex(documents, build_tree=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0acb3-5593-4f00-8eef-315a031fedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_light.query(\"What did the author do after his time at Y Combinator?\", mode=\"summarize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9773497-9aa6-4a16-884a-cd882e63d012",
   "metadata": {},
   "source": [
    "#### [Demo] Build Tree Index with a custom Summary Prompt, directly retrieve answer from root node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab6d3ad-95e1-477a-a0dc-2ce4763ff2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SummaryPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a91a445-6ab2-457c-850e-79c5386129db",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "\n",
    "query_str = \"What did the author do growing up?\"\n",
    "SUMMARY_PROMPT_TMPL = (\n",
    "    \"Context information is below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    f\"answer the question: {query_str}\\n\"\n",
    ")\n",
    "SUMMARY_PROMPT = SummaryPrompt(SUMMARY_PROMPT_TMPL)\n",
    "index_with_query = GPTTreeIndex(documents, summary_template=SUMMARY_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985dad0c-1ede-4576-a4c9-c077b815edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_with_query.save_to_disk(\"index_with_query.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de04fce5-88f9-41b7-87d9-dcde8f84a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_with_query = GPTTreeIndex.load_from_disk(\"index_with_query.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9223ffa8-d49d-4de3-821a-701b2a0352d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly retrieve response from root nodes instead of traversing tree\n",
    "response = index_with_query.query(query_str, mode=\"retrieve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca6970-2f3f-4741-ae98-555db8d3d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6457769-dfaf-4241-ab32-dcf901dde902",
   "metadata": {},
   "source": [
    "## Using GPT Keyword Table Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d59ef6-70b0-47bb-818d-7237a3b7de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTKeywordTableIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f1c67-6d73-4f37-afcf-9e637002fcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build keyword index\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = GPTKeywordTableIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec97988-0190-4df7-b19a-e3130122298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index\n",
    "index.save_to_disk('index_table.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94d0fe0-43c1-41cd-901b-0d748d30f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload index\n",
    "index = GPTKeywordTableIndex.load_from_disk('index_table.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4f686-6825-49cf-a113-d2fdd484de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "response = index.query(\"What did the author do after his time at Y Combinator?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483514d-4ab5-489d-8b99-7250df491ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1bea9-b534-430a-a52b-1f4414957ac9",
   "metadata": {},
   "source": [
    "## Using GPT List Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa8c8c1-7fce-4737-9141-d14fd37a779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTListIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191caa65-a77f-4d8c-b095-4aed61300ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build linked list index\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = GPTListIndex(documents)\n",
    "# save index\n",
    "index.save_to_disk('index_list.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d049d-518d-4ec4-b84f-1fab8aece04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load index from disk\n",
    "index = GPTListIndex.load_from_disk('index_list.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d4bd8-7540-4c6f-8616-ab2d8c6ae2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "response = index.query(\"What did the author do after his time at Y Combinator?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101b979-175f-490e-9b32-27689fe4b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cfce56-853e-431b-888e-946771c3b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba734902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_essay(query_str, document_path):\n",
    "    from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader\n",
    "    documents = SimpleDirectoryReader(document_path).load_data()\n",
    "    index = GPTSimpleVectorIndex(documents)\n",
    "    response = index.query(query_str)\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query_str = \"What did the author do growing up?\"\n",
    "    document_path = \"data\"\n",
    "    print(test_essay(query_str, document_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c61800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
